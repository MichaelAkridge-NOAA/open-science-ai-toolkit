{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "11.8\n",
      "Available GPUs: 2\n",
      "Current GPU: 0\n",
      "GPU Name: Tesla V100-PCIE-32GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.version.cuda)  # Should show 11.8 or similar\n",
    "print(f\"Available GPUs: {torch.cuda.device_count()}\")\n",
    "print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "print(f\"GPU Name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "#helpful commands\n",
    "# run via cmd to check cuda version\n",
    "# nvcc --version\n",
    "# uninstall \n",
    "#(yolo11n_model_train) C:\\Users\\PICHLMRUser>pip uninstall torch torchvision torchaudio\n",
    "# reinstall\n",
    "#(yolo11n_model_train) C:\\Users\\PICHLMRUser>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Ensure the script uses the correct GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # Use only the first GPU\n",
    "\n",
    "# Determine the device to use\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set paths\n",
    "base_path = r\"O:\\OTHER\\AI_DATASETS\\yolo\\datasets\\urchin_datasetv3\"\n",
    "yaml_file_path = os.path.join(base_path, 'data.yaml')\n",
    "\n",
    "# Load the smaller YOLO11 model\n",
    "small_model = YOLO(\"yolo11m.pt\")\n",
    "\n",
    "# Move the model to the correct device\n",
    "small_model.model.to(device)\n",
    "\n",
    "# Freeze the first few layers for the first 10 epochs for better fine-tuning\n",
    "for param in small_model.model.model.parameters():\n",
    "    param.requires_grad = False  # Freeze all layers initially\n",
    "\n",
    "# Training hyperparameters\n",
    "small_model.train(\n",
    "    data=yaml_file_path,\n",
    "    epochs=100,\n",
    "    imgsz=640,\n",
    "    batch=32,  # Adjust batch size based on GPU capacity\n",
    "    lr0=0.001,  # Initial learning rate\n",
    "    lrf=0.0001,  # Final learning rate (used for Cosine Annealing)\n",
    "    optimizer='AdamW',  # Use AdamW optimizer for better performance\n",
    "    device=device,\n",
    "    save_period=10,  # Save model checkpoint every 10 epochs\n",
    "    patience=10,  # Early stopping if no improvement after 10 epochs\n",
    "    augment=True,  # Enable data augmentation\n",
    "    mosaic=True,  # Use mosaic augmentation\n",
    "    mixup=True,   # Use MixUp augmentation\n",
    "    cos_lr=True,  # Cosine annealing learning rate\n",
    "    project='training_logs',  # TensorBoard logging directory\n",
    ")\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Unfreeze all layers after the initial phase\n",
    "for param in small_model.model.model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Save the trained model\n",
    "trained_model_path = os.path.join(base_path, \"yolo11m_urchin_trainedv3.pt\")\n",
    "small_model.save(trained_model_path)\n",
    "print(f\"Trained model saved to {trained_model_path}\")\n",
    "\n",
    "# Save the model weights separately for further use\n",
    "weights_path = os.path.join(base_path, \"yolo11m_urchin_weightsv3.pth\")\n",
    "torch.save(small_model.model.state_dict(), weights_path)\n",
    "print(f\"Weights saved to {weights_path}\")\n",
    "\n",
    "# Evaluate model performance\n",
    "metrics = small_model.val(data=yaml_file_path, device=device)\n",
    "print(metrics)\n",
    "\n",
    "# Export the trained model to ONNX format\n",
    "try:\n",
    "    small_model.export(format=\"onnx\")\n",
    "    print(\"ONNX model exported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"ONNX export failed: {e}\")\n",
    "\n",
    "# Export to TensorFlow Lite\n",
    "try:\n",
    "    small_model.export(format=\"tflite\")\n",
    "    print(\"TFLite model exported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"TFLite export failed: {e}\")\n",
    "\n",
    "# Export to TensorFlow Edge TPU\n",
    "try:\n",
    "    small_model.export(format=\"edgetpu\")\n",
    "    print(\"Edge TPU model exported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Edge TPU export failed: {e}\")\n",
    "\n",
    "# Export to NCNN format\n",
    "try:\n",
    "    small_model.export(format=\"ncnn\")  # Creates .param and .bin files\n",
    "    print(\"NCNN files exported successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"NCNN export failed: {e}\")\n",
    "\n",
    "print(\"Model exports completed where possible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics\n",
    "#!pip install --upgrade --quiet google-cloud-storage\n",
    "#!pip install --upgrade ultralytics\n",
    "#!mkdir -p /content/fish_dataset/images\n",
    "#!mkdir -p /content/fish_dataset/labels\n",
    "#!mkdir -p /content/fish_seg_dataset/images\n",
    "#!mkdir -p /content/fish_seg_dataset/labels\n",
    "#!mkdir -p /content/sam_dataset/\n",
    "#!gsutil -m rsync -r gs://nmfs_odp_pifsc/PIFSC/ESD/ARP/pifsc-ai-data-repository/fish-detection/MOUSS_fish_detection_v1/datasets/small_test_set/images /content/fish_dataset/images\n",
    "#!gsutil -m rsync -r gs://nmfs_odp_pifsc/PIFSC/ESD/ARP/pifsc-ai-data-repository/fish-detection/MOUSS_fish_detection_v1/datasets/small_test_set/labels /content/fish_dataset/labels\n",
    "#!gsutil -m rsync -r gs://nmfs_odp_pifsc/PIFSC/ESD/ARP/pifsc-ai-data-repository/fish-detection/MOUSS_fish_detection_v1/datasets/small_test_set/images /content/fish_dataset/images\n",
    "#!gsutil -m rsync -r gs://nmfs_odp_pifsc/PIFSC/ESD/ARP/pifsc-ai-data-repository/fish-detection/MOUSS_fish_detection_v1/datasets/small_test_set/labels /content/fish_dataset/labels\n",
    "#!gsutil cp gs://nmfs_odp_pifsc/PIFSC/ESD/ARP/pifsc-ai-data-repository/fish-detection/MOUSS_fish_detection_v1/datasets/small_test_set/fish_dataset.yaml /content/\n",
    "\n",
    "#!unzip /content/fish_seg_dataset.zip -d /content/\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO11 segment model\n",
    "model = YOLO(\"yolo11n-seg.pt\")\n",
    "\n",
    "# Train the model\n",
    "results = model.train(data=\"/content/fish_seg_dataset.yaml\", epochs=50, imgsz=416, batch=16, lr0=0.001)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# model save\n",
    "model.save(\"/content/yolo11n_seg_fish_trained.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FR-Y3oydaYZt",
    "YesCg-LraWJe",
    "NFcGFuT0ZxhG"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
